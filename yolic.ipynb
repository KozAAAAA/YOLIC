{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLIC - You Only Look at Interested Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common YOLIC code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from os import listdir\n",
    "from pathlib import Path \n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryPrecision, BinaryRecall\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_COIS = 8\n",
    "NUMBER_OF_CLASSES = 1\n",
    "\n",
    "YOLIC_NET_INPUT_WIDTH = 224\n",
    "YOLIC_NET_INPUT_HEIGHT = 224\n",
    "\n",
    "TRAIN_IMAGE_DIR = Path(\"./CarLast/training/images/\")\n",
    "VAL_IMAGE_DIR = Path(\"./CarLast/validation/images/\")\n",
    "# TEST_IMAGE_DIR = Path(\"../CarLast/test/images/\")\n",
    "\n",
    "TRAIN_LABEL_DIR = Path(\"./CarLast/training/yolic_labels/\")\n",
    "VAL_LABEL_DIR = Path(\"./CarLast/validation/yolic_labels/\")\n",
    "\n",
    "\n",
    "CSV_FILE = Path(\"training_data.csv\")\n",
    "\n",
    "YOLIC_MODEL_PATH = Path(\"yolic_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YolicDataset(Dataset):\n",
    "    \"\"\"Yolic Dataset class used to load images and label\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: Path,\n",
    "        label_dir: Path,\n",
    "        transform=None,\n",
    "    ):\n",
    "        \"\"\"Initialize YolicDataset class\"\"\"\n",
    "        self._image_dir = image_dir\n",
    "        self._label_dir = label_dir\n",
    "        self._image_names = listdir(image_dir)\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the length of the dataset\"\"\"\n",
    "        return len(self._image_names)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[Any, torch.Tensor, str]:\n",
    "        \"\"\"Return image, label and image name\"\"\"\n",
    "\n",
    "        image_name = Path(self._image_names[index])\n",
    "\n",
    "        image_path = self._image_dir / image_name\n",
    "        label_path = self._label_dir / image_name.with_suffix(\".txt\")\n",
    "\n",
    "        label = torch.from_numpy(np.loadtxt(fname=label_path, dtype=np.float32))\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self._transform is not None:\n",
    "            image = self._transform(image)\n",
    "        # add random augmentation here\n",
    "        filename = image_name.stem\n",
    "        return image, label, filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolic_net(weights=None) -> nn.Module:\n",
    "    \"\"\"Returns a Yolic model\"\"\"\n",
    "\n",
    "    model = mobilenet_v2(weights=weights)\n",
    "    model.classifier = nn.Sequential(  # Swap out the classifier\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(\n",
    "            in_features=model.last_channel,\n",
    "            out_features=(NUMBER_OF_COIS * (NUMBER_OF_CLASSES + 1)),\n",
    "        ),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train YOLIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Train and validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    loss_fn: Any,\n",
    "    accuracy_fn: Any,\n",
    "    f1_score_fn: Any,\n",
    "    precision_fn: Any,\n",
    "    recall_fn: Any,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    data_loader: DataLoader,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Perform a single training step\"\"\"\n",
    "\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    with alive_bar(len(data_loader)) as bar:\n",
    "        for images, labels, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            output = torch.sigmoid(output)\n",
    "\n",
    "            # calculate metrics\n",
    "            labels = labels.to(torch.int64)\n",
    "            for i in range(len(output)):\n",
    "                accuracy_fn.update(output[i], labels[i])\n",
    "                f1_score_fn.update(output[i], labels[i])\n",
    "                precision_fn.update(output[i], labels[i])\n",
    "                recall_fn.update(output[i], labels[i])\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update the progress bar\n",
    "            bar()\n",
    "\n",
    "    train_loss = train_loss / len(data_loader)\n",
    "    train_accuracy = accuracy_fn.compute().item()\n",
    "    train_f1_score = f1_score_fn.compute().item()\n",
    "    train_precision = precision_fn.compute().item()\n",
    "    train_recall = recall_fn.compute().item()\n",
    "\n",
    "    accuracy_fn.reset()\n",
    "    f1_score_fn.reset()\n",
    "    precision_fn.reset()\n",
    "    recall_fn.reset()\n",
    "\n",
    "    return train_loss, train_accuracy, train_f1_score, train_precision, train_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(\n",
    "    model: nn.Module,\n",
    "    loss_fn: Any,\n",
    "    accuracy_fn: Any,\n",
    "    f1_score_fn: Any,\n",
    "    precision_fn: Any,\n",
    "    recall_fn: Any,\n",
    "    device: torch.device,\n",
    "    data_loader: DataLoader,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Perform validation of a model\"\"\"\n",
    "\n",
    "    validation_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            output = torch.sigmoid(output)\n",
    "\n",
    "            # calculate metrics\n",
    "            labels = labels.to(torch.int64)\n",
    "            for i in range(len(output)):\n",
    "                accuracy_fn.update(output[i], labels[i])\n",
    "                f1_score_fn.update(output[i], labels[i])\n",
    "                precision_fn.update(output[i], labels[i])\n",
    "                recall_fn.update(output[i], labels[i])\n",
    "\n",
    "\n",
    "    validation_loss = validation_loss / len(data_loader)\n",
    "    validation_accuracy = accuracy_fn.compute().item()\n",
    "    validation_f1_score = f1_score_fn.compute().item()\n",
    "    validation_precision = precision_fn.compute().item()\n",
    "    validation_recall = recall_fn.compute().item()\n",
    "\n",
    "    accuracy_fn.reset()\n",
    "    \n",
    "    return (\n",
    "        validation_loss,\n",
    "        validation_accuracy,\n",
    "        validation_f1_score,\n",
    "        validation_precision,\n",
    "        validation_recall,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 40\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 YOLIC train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolic_train():\n",
    "    \"\"\"Main function\"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device}\")\n",
    "\n",
    "    # Declare model, loss function, optimizer and scheduler\n",
    "    model = yolic_net(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[20, 25], gamma=0.1)\n",
    "\n",
    "    # Declare transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        (\n",
    "            [\n",
    "                transforms.Resize((YOLIC_NET_INPUT_HEIGHT, YOLIC_NET_INPUT_WIDTH)),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    validation_transform = transforms.Compose(\n",
    "        (\n",
    "            [\n",
    "                transforms.Resize((YOLIC_NET_INPUT_HEIGHT, YOLIC_NET_INPUT_WIDTH)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = YolicDataset(TRAIN_IMAGE_DIR, TRAIN_LABEL_DIR, train_transform)\n",
    "    validation_dataset = YolicDataset(VAL_IMAGE_DIR, VAL_LABEL_DIR, validation_transform)\n",
    "    print(f\"Train data: {len(train_dataset)}\")\n",
    "    print(f\"Validation data: {len(validation_dataset)}\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    "    )\n",
    "    validation_loader = DataLoader(\n",
    "        validation_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    train_f1_score = []\n",
    "    train_precision = []\n",
    "    train_recall = []\n",
    "\n",
    "    validation_loss = []\n",
    "    validation_accuracy = []\n",
    "    validation_f1_score = []\n",
    "    validation_precision = []\n",
    "    validation_recall = []\n",
    "\n",
    "    smallest_validation_loss = float(\"inf\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch: {epoch+1} / {EPOCHS}\")\n",
    "        (\n",
    "            epoch_train_loss,\n",
    "            epoch_train_accuracy,\n",
    "            epoch_train_f1_score,\n",
    "            epoch_train_precision,\n",
    "            epoch_train_recall,\n",
    "        ) = train_step(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=BinaryAccuracy(),\n",
    "            f1_score_fn=BinaryF1Score(),\n",
    "            precision_fn=BinaryPrecision(),\n",
    "            recall_fn=BinaryRecall(),\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            data_loader=train_loader,\n",
    "        )\n",
    "\n",
    "        (\n",
    "            epoch_validation_loss,\n",
    "            epoch_validation_accuracy,\n",
    "            epoch_validation_f1_score,\n",
    "            epoch_validation_precision,\n",
    "            epoch_validation_recall,\n",
    "        ) = validation_step(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=BinaryAccuracy(),\n",
    "            f1_score_fn=BinaryF1Score(),\n",
    "            precision_fn=BinaryPrecision(),\n",
    "            recall_fn=BinaryRecall(),\n",
    "            device=device,\n",
    "            data_loader=validation_loader,\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f\"Train loss: {epoch_train_loss},\",\n",
    "            f\"Train accuracy: {epoch_train_accuracy}\",\n",
    "            f\"Train f1 score: {epoch_train_f1_score}\",\n",
    "            f\"Train precision: {epoch_train_precision}\",\n",
    "            f\"Train recall: {epoch_train_recall}\",\n",
    "        )\n",
    "        print(\n",
    "            f\"Validation loss: {epoch_validation_loss},\",\n",
    "            f\"Validation accuracy: {epoch_validation_accuracy}\",\n",
    "            f\"Validation f1 score: {epoch_validation_f1_score}\",\n",
    "            f\"Validation precision: {epoch_validation_precision}\",\n",
    "            f\"Validation recall: {epoch_validation_recall}\",\n",
    "        )\n",
    "\n",
    "        # TODO: Change metric to precision, recall or f1 score\n",
    "        if epoch_validation_loss < smallest_validation_loss:\n",
    "            smallest_validation_loss = epoch_validation_loss\n",
    "            torch.save(model.state_dict(), YOLIC_MODEL_PATH)\n",
    "            print(f\"Saved new weights as {YOLIC_MODEL_PATH}\")\n",
    "\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        train_accuracy.append(epoch_train_accuracy)\n",
    "        train_f1_score.append(epoch_train_f1_score)\n",
    "        train_precision.append(epoch_train_precision)\n",
    "        train_recall.append(epoch_train_recall)\n",
    "\n",
    "\n",
    "        validation_loss.append(epoch_validation_loss)\n",
    "        validation_accuracy.append(epoch_validation_accuracy)\n",
    "        validation_f1_score.append(epoch_validation_f1_score)\n",
    "        validation_precision.append(epoch_validation_precision)\n",
    "        validation_recall.append(epoch_validation_recall)\n",
    "\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"validation_loss\": validation_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"validation_accuracy\": validation_accuracy,\n",
    "                \"train_f1_score\": train_f1_score,\n",
    "                \"validation_f1_score\": validation_f1_score,\n",
    "                \"train_precision\": train_precision,\n",
    "                \"validation_precision\": validation_precision,\n",
    "                \"train_recall\": train_recall,\n",
    "                \"validation_recall\": validation_recall,\n",
    "            }\n",
    "        ).to_csv(CSV_FILE, index=False)\n",
    "        print(f\"Added new metrics to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Start YOLIC training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolic_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Plot training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Extract the columns\n",
    "train_loss = data['train_loss']\n",
    "validation_loss = data['validation_loss']\n",
    "train_accuracy = data['train_accuracy']\n",
    "validation_accuracy = data['validation_accuracy']\n",
    "\n",
    "# Create the first plot for loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Train Loss', color='blue')\n",
    "plt.plot(validation_loss, label='Validation Loss', color='red')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Create the second plot for accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracy, label='Train Accuracy', color='green')\n",
    "plt.plot(validation_accuracy, label='Validation Accuracy', color='purple')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Create the third plot for f1 score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(data['train_f1_score'], label='Train F1 Score', color='blue')\n",
    "plt.plot(data['validation_f1_score'], label='Validation F1 Score', color='red')\n",
    "plt.title('F1 Score Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "# Create the fourth plot for precision\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(data['train_precision'], label='Train Precision', color='green')\n",
    "plt.plot(data['validation_precision'], label='Validation Precision', color='purple')\n",
    "plt.title('Precision Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Create the fifth plot for recall\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(data['train_recall'], label='Train Recall', color='blue')\n",
    "plt.plot(data['validation_recall'], label='Validation Recall', color='red')\n",
    "plt.title('Recall Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Adjust layout and show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test YOLIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELLS_FILE = Path('./cell_design.json')\n",
    "GENERATED_IMAGES_DIR = Path('./generated_images/')\n",
    "COLOR_LABELS = {\n",
    "    \"Background\": (255, 255, 255),\n",
    "    \"Vehicle\": (255, 0, 0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(l, n):\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_image(image: torch.tensor, output: torch.tensor):\n",
    "    image = np.array(image)\n",
    "    output = to_matrix(output.tolist(), NUMBER_OF_CLASSES + 1)\n",
    "    print(output)\n",
    "\n",
    "    with open(str(CELLS_FILE)) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        cois_data = data[\"COIs\"]\n",
    "        labels = data[\"Labels\"][\"LabelList\"]\n",
    "\n",
    "    labels.insert(0, \"Background\")\n",
    "\n",
    "    # generate masks for every cell of interests\n",
    "    for idx, coi in enumerate(cois_data, -1):\n",
    "        if coi == \"COINumber\":\n",
    "            continue\n",
    "        # get label coi label\n",
    "        label = labels[output[idx].index(1)]\n",
    "        color = COLOR_LABELS[label]\n",
    "\n",
    "        # reshape array of coordinates\n",
    "        poly_cords = np.array(cois_data[coi][1:])\n",
    "        num_cords = poly_cords.size // 2\n",
    "        poly_cords = poly_cords.reshape(num_cords, 2)\n",
    "        poly_cords = np.array(\n",
    "            [\n",
    "                np.multiply(cords, (YOLIC_NET_INPUT_HEIGHT, YOLIC_NET_INPUT_WIDTH))\n",
    "                for cords in poly_cords\n",
    "            ]\n",
    "        ).astype(np.int32)\n",
    "\n",
    "        # get mask\n",
    "        if cois_data[coi][0] == \"polygon\":\n",
    "            cv2.polylines(image, [poly_cords], True, color, 2)\n",
    "        else:\n",
    "            [[x, y], [w, h]] = poly_cords\n",
    "            cv2.rectangle(image, [x, y], [x + w, y + h], color, 2)\n",
    "\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process file with cells of interests layout\n",
    "def yolic_test():\n",
    "    \"\"\"Test the model on the test dataset and save the predicted images\"\"\"\n",
    "    model = yolic_net()\n",
    "    model.load_state_dict(torch.load(YOLIC_MODEL_PATH))\n",
    "    transform = transforms.Compose(\n",
    "        (\n",
    "            [\n",
    "                transforms.Resize((YOLIC_NET_INPUT_HEIGHT, YOLIC_NET_INPUT_WIDTH)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    dataset = YolicDataset(VAL_IMAGE_DIR, VAL_LABEL_DIR)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, _, _ in dataset:\n",
    "            transformed_image = transform(image)\n",
    "            transformed_image = transformed_image.unsqueeze(0)\n",
    "            output = model(transformed_image)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = torch.where(output >= 0.5, torch.ones_like(output), torch.zeros_like(output)).to(torch.int64)\n",
    "            print_generated_image(image, output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolic_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
