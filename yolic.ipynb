{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLIC - You Only Look at Interested Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common YOLIC code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from os import listdir\n",
    "from pathlib import Path \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_COIS = 104\n",
    "NUMBER_OF_CLASSES = 11\n",
    "\n",
    "YOLIC_NET_INPUT_WIDTH = 224\n",
    "YOLIC_NET_INPUT_HEIGHT = 224\n",
    "\n",
    "TRAIN_IMAGE_DIR = Path(\"./data/images/train/outdoor/\")\n",
    "VAL_IMAGE_DIR = Path(\"./data/images/val/outdoor/\")\n",
    "TEST_IMAGE_DIR = Path(\"./data/images/test/outdoor/\")\n",
    "\n",
    "LABEL_DIR = Path(\"./data/labels/\")\n",
    "\n",
    "CSV_FILE = Path(\"training_data.csv\")\n",
    "\n",
    "YOLIC_MODEL_PATH = Path(\"yolic_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YolicDataset(Dataset):\n",
    "    \"\"\"Yolic Dataset class used to load images and label\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: Path,\n",
    "        label_dir: Path,\n",
    "        transform=None,\n",
    "    ):\n",
    "        \"\"\"Initialize YolicDataset class\"\"\"\n",
    "        self._image_dir = image_dir\n",
    "        self._label_dir = label_dir\n",
    "        self._image_names = listdir(image_dir)\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the length of the dataset\"\"\"\n",
    "        return len(self._image_names)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[Any, torch.Tensor, str]:\n",
    "        \"\"\"Return image, label and image name\"\"\"\n",
    "\n",
    "        image_name = Path(self._image_names[index])\n",
    "\n",
    "        image_path = self._image_dir / image_name\n",
    "        label_path = self._label_dir / image_name.with_suffix(\".txt\")\n",
    "\n",
    "        label = torch.from_numpy(np.loadtxt(fname=label_path, dtype=np.float32))\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self._transform is not None:\n",
    "            image = self._transform(image)\n",
    "        # add random augmentation here\n",
    "        filename = image_name.stem\n",
    "        return image, label, filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolic_net(weights=None) -> nn.Module:\n",
    "    \"\"\"Returns a Yolic model\"\"\"\n",
    "\n",
    "    model = mobilenet_v2(weights=weights)\n",
    "    model.classifier = nn.Sequential(  # Swap out the classifier\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(\n",
    "            in_features=model.last_channel,\n",
    "            out_features=(NUMBER_OF_COIS * (NUMBER_OF_CLASSES + 1)),\n",
    "        ),\n",
    "        # nn.Sigmoid(),  # Should i use it here or durning the evaluation?\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train YOLIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function has been copied from the original yolic repository,\n",
    "# not fully understood yet\n",
    "def yolic_accuracy(\n",
    "    preds: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> float:\n",
    "    \"\"\"Calculate accuracy of the model\"\"\"\n",
    "    accuracy = 0.0\n",
    "    for i, pred in enumerate(preds):\n",
    "        target = targets[i]\n",
    "        target = torch.Tensor.cpu(target)\n",
    "        pred = torch.Tensor.cpu(pred)\n",
    "\n",
    "        pred = torch.round(pred).detach().numpy().astype(np.int64)\n",
    "        target = target.detach().numpy()\n",
    "        pred = np.reshape(pred, (NUMBER_OF_COIS * (NUMBER_OF_CLASSES + 1), 1)).flatten()\n",
    "        target = np.reshape(\n",
    "            target, (NUMBER_OF_COIS * (NUMBER_OF_CLASSES + 1), 1)\n",
    "        ).flatten()\n",
    "        num = 0\n",
    "        single_accuracy = 0.0\n",
    "        for cell in range(\n",
    "            0, (NUMBER_OF_COIS * (NUMBER_OF_CLASSES + 1)), NUMBER_OF_CLASSES + 1\n",
    "        ):\n",
    "            if (\n",
    "                target[cell : cell + NUMBER_OF_CLASSES + 1]\n",
    "                == pred[cell : cell + NUMBER_OF_CLASSES + 1]\n",
    "            ).all():\n",
    "                num = num + 1\n",
    "\n",
    "            single_accuracy = num / NUMBER_OF_COIS\n",
    "        accuracy += single_accuracy\n",
    "    accuracy = accuracy / len(preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Train and validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    loss_fn: Any,\n",
    "    accuracy_fn: Any,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    data_loader: DataLoader,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Perform a single training step\"\"\"\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    model.train()\n",
    "    with alive_bar(len(data_loader)) as bar:\n",
    "        for images, labels, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            output = torch.sigmoid(output)  # should i use it here or in the model?\n",
    "            train_accuracy += accuracy_fn(output, labels)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update the progress bar\n",
    "            bar()\n",
    "\n",
    "    train_loss = train_loss / len(data_loader)\n",
    "    train_accuracy = train_accuracy / len(data_loader)\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(\n",
    "    model: nn.Module,\n",
    "    loss_fn: Any,\n",
    "    accuracy_fn: Any,\n",
    "    device: torch.device,\n",
    "    data_loader: DataLoader,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Perform validation of a model\"\"\"\n",
    "\n",
    "    validation_loss = 0.0\n",
    "    validation_accuracy = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            output = torch.sigmoid(output)  # should i use it here or in the model?\n",
    "            validation_accuracy += accuracy_fn(output, labels)\n",
    "\n",
    "    validation_loss = validation_loss / len(data_loader)\n",
    "    validation_accuracy = validation_accuracy / len(data_loader)\n",
    "    return validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 55\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 YOLIC train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolic_train():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device}\")\n",
    "\n",
    "    # Declare model, loss function, optimizer and scheduler\n",
    "    model = yolic_net(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[100, 125], gamma=0.1)\n",
    "\n",
    "    # Declare transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        (\n",
    "            [\n",
    "                transforms.Resize((YOLIC_NET_INPUT_HEIGHT, YOLIC_NET_INPUT_WIDTH)),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    validation_transform = transforms.Compose(\n",
    "        (\n",
    "            [\n",
    "                transforms.Resize((YOLIC_NET_INPUT_HEIGHT, YOLIC_NET_INPUT_WIDTH)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = YolicDataset(TRAIN_IMAGE_DIR, LABEL_DIR, train_transform)\n",
    "    validation_dataset = YolicDataset(VAL_IMAGE_DIR, LABEL_DIR, validation_transform)\n",
    "    print(f\"Train data: {len(train_dataset)}\")\n",
    "    print(f\"Validation data: {len(validation_dataset)}\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    "    )\n",
    "    validation_loader = DataLoader(\n",
    "        validation_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    train_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    previous_validation_accuracy = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch: {epoch+1} / {EPOCHS}\")\n",
    "        epoch_train_loss, epoch_train_accuracy = train_step(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=yolic_accuracy,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            data_loader=train_loader,\n",
    "        )\n",
    "        epoch_validation_loss, epoch_validation_accuracy = validation_step(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=yolic_accuracy,\n",
    "            device=device,\n",
    "            data_loader=validation_loader,\n",
    "        )\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f\"Train loss: {epoch_train_loss},\",\n",
    "            f\"Train accuracy: {epoch_train_accuracy}\",\n",
    "        )\n",
    "        print(\n",
    "            f\"Validation loss: {epoch_validation_loss},\",\n",
    "            f\"Validation accuracy: {epoch_validation_accuracy}\",\n",
    "        )\n",
    "\n",
    "        # TODO: Change metric to precision, recall or f1 score\n",
    "        if epoch_validation_accuracy > previous_validation_accuracy:\n",
    "            torch.save(model.state_dict(), YOLIC_MODEL_PATH)\n",
    "            previous_validation_accuracy = epoch_validation_accuracy\n",
    "            print(f\"Saved new weights as {YOLIC_MODEL_PATH}\")\n",
    "\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        validation_loss.append(epoch_validation_loss)\n",
    "        train_accuracy.append(epoch_train_accuracy)\n",
    "        validation_accuracy.append(epoch_validation_accuracy)\n",
    "\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"validation_loss\": validation_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"validation_accuracy\": validation_accuracy,\n",
    "            }\n",
    "        ).to_csv(CSV_FILE, index=False)\n",
    "        print(f\"Added new metrics to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Start YOLIC training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolic_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Plot training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Extract the columns\n",
    "train_loss = data['train_loss']\n",
    "validation_loss = data['validation_loss']\n",
    "train_accuracy = data['train_accuracy']\n",
    "validation_accuracy = data['validation_accuracy']\n",
    "\n",
    "# Create the first plot for loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Train Loss', color='blue')\n",
    "plt.plot(validation_loss, label='Validation Loss', color='red')\n",
    "plt.title('Loss Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Create the second plot for accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracy, label='Train Accuracy', color='green')\n",
    "plt.plot(validation_accuracy, label='Validation Accuracy', color='purple')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout and show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test YOLIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Generated images directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_IMAGE_DIR = Path(\"generated_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Cells and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_list = [\n",
    "    (288, 166),\n",
    "    (322, 166),\n",
    "    (356, 166),\n",
    "    (390, 166),\n",
    "    (424, 166),\n",
    "    (458, 166),\n",
    "    (492, 166),\n",
    "    (526, 166),\n",
    "    (220, 200),\n",
    "    (254, 200),\n",
    "    (288, 200),\n",
    "    (322, 200),\n",
    "    (356, 200),\n",
    "    (390, 200),\n",
    "    (424, 200),\n",
    "    (458, 200),\n",
    "    (492, 200),\n",
    "    (526, 200),\n",
    "    (560, 200),\n",
    "    (594, 200),\n",
    "    (220, 234),\n",
    "    (254, 234),\n",
    "    (288, 234),\n",
    "    (322, 234),\n",
    "    (356, 234),\n",
    "    (390, 234),\n",
    "    (424, 234),\n",
    "    (458, 234),\n",
    "    (492, 234),\n",
    "    (526, 234),\n",
    "    (560, 234),\n",
    "    (594, 234),\n",
    "    (628, 234),\n",
    "    (254, 268),\n",
    "    (288, 268),\n",
    "    (322, 268),\n",
    "    (356, 268),\n",
    "    (390, 268),\n",
    "    (424, 268),\n",
    "    (458, 268),\n",
    "    (492, 268),\n",
    "    (526, 268),\n",
    "    (560, 268),\n",
    "    (594, 268),\n",
    "    (628, 268),\n",
    "    (0, 268),\n",
    "    (53, 268),\n",
    "    (106, 268),\n",
    "    (159, 268),\n",
    "    (212, 268),\n",
    "    (265, 268),\n",
    "    (318, 268),\n",
    "    (371, 268),\n",
    "    (424, 268),\n",
    "    (477, 268),\n",
    "    (530, 268),\n",
    "    (583, 268),\n",
    "    (636, 268),\n",
    "    (689, 268),\n",
    "    (742, 268),\n",
    "    (795, 268),\n",
    "    (0, 321),\n",
    "    (53, 321),\n",
    "    (106, 321),\n",
    "    (159, 321),\n",
    "    (212, 321),\n",
    "    (265, 321),\n",
    "    (318, 321),\n",
    "    (371, 321),\n",
    "    (424, 321),\n",
    "    (477, 321),\n",
    "    (530, 321),\n",
    "    (583, 321),\n",
    "    (636, 321),\n",
    "    (689, 321),\n",
    "    (742, 321),\n",
    "    (795, 321),\n",
    "    (848, 321),\n",
    "    (0, 374),\n",
    "    (53, 374),\n",
    "    (106, 374),\n",
    "    (159, 374),\n",
    "    (212, 374),\n",
    "    (265, 374),\n",
    "    (318, 374),\n",
    "    (371, 374),\n",
    "    (424, 374),\n",
    "    (477, 374),\n",
    "    (530, 374),\n",
    "    (583, 374),\n",
    "    (636, 374),\n",
    "    (689, 374),\n",
    "    (742, 374),\n",
    "    (795, 374),\n",
    "    (848, 374),\n",
    "    (0, 427),\n",
    "    (53, 427),\n",
    "    (106, 427),\n",
    "    (159, 427),\n",
    "    (212, 427),\n",
    "    (265, 427),\n",
    "    (318, 427),\n",
    "    (371, 427),\n",
    "    (424, 427),\n",
    "    (477, 427),\n",
    "    (530, 427),\n",
    "    (583, 427),\n",
    "    (636, 427),\n",
    "    (689, 427),\n",
    "    (742, 427),\n",
    "    (795, 427),\n",
    "    (848, 427),\n",
    "    (0, 480),\n",
    "    (53, 480),\n",
    "    (106, 480),\n",
    "    (159, 480),\n",
    "    (212, 480),\n",
    "    (265, 480),\n",
    "    (318, 480),\n",
    "    (371, 480),\n",
    "    (424, 480),\n",
    "    (477, 480),\n",
    "    (530, 480),\n",
    "    (583, 480),\n",
    "    (636, 480),\n",
    "    (689, 480),\n",
    "    (742, 480),\n",
    "    (795, 480),\n",
    "    (848, 480),\n",
    "    (184, 0),\n",
    "    (244, 0),\n",
    "    (304, 0),\n",
    "    (364, 0),\n",
    "    (424, 0),\n",
    "    (484, 0),\n",
    "    (544, 0),\n",
    "    (604, 0),\n",
    "    (244, 60),\n",
    "    (304, 60),\n",
    "    (364, 60),\n",
    "    (424, 60),\n",
    "    (484, 60),\n",
    "    (544, 60),\n",
    "    (604, 60),\n",
    "    (664, 60),\n",
    "]\n",
    "cell_list = [\n",
    "    [points_list[0], points_list[11]],\n",
    "    [points_list[1], points_list[12]],\n",
    "    [points_list[2], points_list[13]],\n",
    "    [points_list[3], points_list[14]],\n",
    "    [points_list[4], points_list[15]],\n",
    "    [points_list[5], points_list[16]],\n",
    "    [points_list[6], points_list[17]],\n",
    "    [points_list[7], points_list[18]],\n",
    "    [points_list[8], points_list[21]],\n",
    "    [points_list[9], points_list[22]],\n",
    "    [points_list[10], points_list[23]],\n",
    "    [points_list[11], points_list[24]],\n",
    "    [points_list[12], points_list[25]],\n",
    "    [points_list[13], points_list[26]],\n",
    "    [points_list[14], points_list[27]],\n",
    "    [points_list[15], points_list[28]],\n",
    "    [points_list[16], points_list[29]],\n",
    "    [points_list[17], points_list[30]],\n",
    "    [points_list[18], points_list[31]],\n",
    "    [points_list[19], points_list[32]],\n",
    "    [points_list[20], points_list[33]],\n",
    "    [points_list[21], points_list[34]],\n",
    "    [points_list[22], points_list[35]],\n",
    "    [points_list[23], points_list[36]],\n",
    "    [points_list[24], points_list[37]],\n",
    "    [points_list[25], points_list[38]],\n",
    "    [points_list[26], points_list[39]],\n",
    "    [points_list[27], points_list[40]],\n",
    "    [points_list[28], points_list[41]],\n",
    "    [points_list[29], points_list[42]],\n",
    "    [points_list[30], points_list[43]],\n",
    "    [points_list[31], points_list[44]],\n",
    "    [points_list[45], points_list[62]],\n",
    "    [points_list[46], points_list[63]],\n",
    "    [points_list[47], points_list[64]],\n",
    "    [points_list[48], points_list[65]],\n",
    "    [points_list[49], points_list[66]],\n",
    "    [points_list[50], points_list[67]],\n",
    "    [points_list[51], points_list[68]],\n",
    "    [points_list[52], points_list[69]],\n",
    "    [points_list[53], points_list[70]],\n",
    "    [points_list[54], points_list[71]],\n",
    "    [points_list[55], points_list[72]],\n",
    "    [points_list[56], points_list[73]],\n",
    "    [points_list[57], points_list[74]],\n",
    "    [points_list[58], points_list[75]],\n",
    "    [points_list[59], points_list[76]],\n",
    "    [points_list[60], points_list[77]],\n",
    "    [points_list[61], points_list[79]],\n",
    "    [points_list[62], points_list[80]],\n",
    "    [points_list[63], points_list[81]],\n",
    "    [points_list[64], points_list[82]],\n",
    "    [points_list[65], points_list[83]],\n",
    "    [points_list[66], points_list[84]],\n",
    "    [points_list[67], points_list[85]],\n",
    "    [points_list[68], points_list[86]],\n",
    "    [points_list[69], points_list[87]],\n",
    "    [points_list[70], points_list[88]],\n",
    "    [points_list[71], points_list[89]],\n",
    "    [points_list[72], points_list[90]],\n",
    "    [points_list[73], points_list[91]],\n",
    "    [points_list[74], points_list[92]],\n",
    "    [points_list[75], points_list[93]],\n",
    "    [points_list[76], points_list[94]],\n",
    "    [points_list[78], points_list[96]],\n",
    "    [points_list[79], points_list[97]],\n",
    "    [points_list[80], points_list[98]],\n",
    "    [points_list[81], points_list[99]],\n",
    "    [points_list[82], points_list[100]],\n",
    "    [points_list[83], points_list[101]],\n",
    "    [points_list[84], points_list[102]],\n",
    "    [points_list[85], points_list[103]],\n",
    "    [points_list[86], points_list[104]],\n",
    "    [points_list[87], points_list[105]],\n",
    "    [points_list[88], points_list[106]],\n",
    "    [points_list[89], points_list[107]],\n",
    "    [points_list[90], points_list[108]],\n",
    "    [points_list[91], points_list[109]],\n",
    "    [points_list[92], points_list[110]],\n",
    "    [points_list[93], points_list[111]],\n",
    "    [points_list[95], points_list[113]],\n",
    "    [points_list[96], points_list[114]],\n",
    "    [points_list[97], points_list[115]],\n",
    "    [points_list[98], points_list[116]],\n",
    "    [points_list[99], points_list[117]],\n",
    "    [points_list[100], points_list[118]],\n",
    "    [points_list[101], points_list[119]],\n",
    "    [points_list[102], points_list[120]],\n",
    "    [points_list[103], points_list[121]],\n",
    "    [points_list[104], points_list[122]],\n",
    "    [points_list[105], points_list[123]],\n",
    "    [points_list[106], points_list[124]],\n",
    "    [points_list[107], points_list[125]],\n",
    "    [points_list[108], points_list[126]],\n",
    "    [points_list[109], points_list[127]],\n",
    "    [points_list[110], points_list[128]],\n",
    "    [points_list[129], points_list[137]],\n",
    "    [points_list[130], points_list[138]],\n",
    "    [points_list[131], points_list[139]],\n",
    "    [points_list[132], points_list[140]],\n",
    "    [points_list[133], points_list[141]],\n",
    "    [points_list[134], points_list[142]],\n",
    "    [points_list[135], points_list[143]],\n",
    "    [points_list[136], points_list[144]],\n",
    "]\n",
    "class_names = [\n",
    "    \"Bump\",\n",
    "    \"Column\",\n",
    "    \"Dent\",\n",
    "    \"Fence\",\n",
    "    \"Creature\",\n",
    "    \"Vehicle\",\n",
    "    \"Wall\",\n",
    "    \"Weed\",\n",
    "    \"ZebraCrossing\",\n",
    "    \"TrafficCone\",\n",
    "    \"TrafficSign\",\n",
    "    \"Road\",\n",
    "    \"Background\",\n",
    "]\n",
    "color_box = [\n",
    "    (10, 249, 72),\n",
    "    (151, 157, 255),\n",
    "    (134, 219, 61),\n",
    "    (52, 147, 26),\n",
    "    (29, 178, 255),\n",
    "    (31, 112, 255),\n",
    "    (49, 210, 207),\n",
    "    (23, 204, 146),\n",
    "    (56, 56, 255),\n",
    "    (187, 212, 0),\n",
    "    (168, 153, 44),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Draw rectangles function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_plot(frame, original, output):\n",
    "    orig = original.detach().numpy()\n",
    "    output = output.detach().numpy()\n",
    "    pred = np.where(output > 0.001, 1, 0).tolist()\n",
    "    cell = 0\n",
    "    normal = np.asarray([0] * NUMBER_OF_CLASSES + [1])\n",
    "    for rect in cell_list:\n",
    "        x1, y1 = rect[0]\n",
    "        x2, y2 = rect[1]\n",
    "        cv2.rectangle(\n",
    "            frame, tuple(rect[0]), tuple(rect[1]), color=(0, 0, 0), thickness=3\n",
    "        )  # print black rectangles if background detected\n",
    "        each = pred[cell : cell + NUMBER_OF_CLASSES + 1]\n",
    "        eachScore = output[cell : cell + NUMBER_OF_CLASSES + 1]\n",
    "        # each = orig[cell : cell + NUMBER_OF_CLASSES + 1]\n",
    "        if not (each == normal).all():\n",
    "            index = [i for i, x in enumerate(each) if x == 1]\n",
    "            if len(index) == 0:\n",
    "                index.append(eachScore.argmax())\n",
    "                if eachScore.argmax() == NUMBER_OF_CLASSES:\n",
    "                    continue\n",
    "            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            poly_area = (x2 - x1) * (y2 - y1)\n",
    "            default_text_scale = 0.4\n",
    "            texts = []\n",
    "            max_text_len = len(index)\n",
    "            if max_text_len > 1:\n",
    "                text_scale = default_text_scale * min(\n",
    "                    1, np.sqrt(poly_area) / max_text_len\n",
    "                )\n",
    "            else:\n",
    "                text_scale = min(max(poly_area / 10000, 0.3), 0.6)\n",
    "            for i in index:\n",
    "                text_size, _ = cv2.getTextSize(\n",
    "                    class_names[i], cv2.FONT_HERSHEY_SIMPLEX, text_scale, 2\n",
    "                )\n",
    "                texts.append((class_names[i], text_size, text_scale))\n",
    "            text_origin = [center_x, center_y - sum(text[1][1] for text in texts) // 2]\n",
    "            line_spacing = 0.7\n",
    "            color = color_box[index[0]]\n",
    "            for text, text_size, text_scale in texts:\n",
    "                text_origin[0] = center_x - text_size[0] // 2\n",
    "                text_origin[1] += int(text_size[1] * line_spacing)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    text,\n",
    "                    tuple(text_origin),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    text_scale,\n",
    "                    (255, 255, 255),\n",
    "                    1,\n",
    "                )  # Add text in the middle of the rectangle\n",
    "                text_origin[1] += int(text_size[1] * line_spacing)\n",
    "            cv2.rectangle(\n",
    "                frame, tuple(rect[0]), tuple(rect[1]), color=color, thickness=3\n",
    "            )\n",
    "        cell += NUMBER_OF_CLASSES + 1\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 YOLIC test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolic_test():\n",
    "    \"\"\"Test the model on the test dataset and save the predicted images\"\"\"\n",
    "    GENERATED_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = yolic_net()\n",
    "    model.load_state_dict(torch.load(YOLIC_MODEL_PATH))\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        (\n",
    "            [\n",
    "                transforms.Resize((YOLIC_NET_INPUT_HEIGHT, YOLIC_NET_INPUT_WIDTH)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    test_dataset = YolicDataset(TEST_IMAGE_DIR, LABEL_DIR)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target, filename in test_dataset:\n",
    "            transformed_image = transform(image)\n",
    "            transformed_image = transformed_image.unsqueeze(0)\n",
    "            output = model(transformed_image)\n",
    "            output = torch.sigmoid(output)\n",
    "\n",
    "            image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "            frame = pred_plot(\n",
    "                image, target, output[0]\n",
    "            )  # shouldnt it take tensors as inputs?\n",
    "\n",
    "            file_path = GENERATED_IMAGE_DIR / Path(filename).with_suffix(\".png\")\n",
    "            print(f\"Saving {file_path}...\")\n",
    "            cv2.imwrite(str(file_path), frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Start YOLIC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolic_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = 259\n",
    "image_path = Path(GENERATED_IMAGE_DIR / listdir(GENERATED_IMAGE_DIR)[image_num]) \n",
    "image = mpimg.imread(image_path)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
